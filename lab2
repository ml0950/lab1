 
import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.datasets import make_classification 
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import StandardScaler 
from sklearn.linear_model import LogisticRegression  
from sklearn.metrics import accuracy_score, log_loss 
# Data creation and preprocessing 
X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, 
random_state=3537) 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=3537) 
scaler = StandardScaler() 
X_train, X_test = scaler.fit_transform(X_train), scaler.transform(X_test) 
# Model training and predictions 
model = LogisticRegression().fit(X_train, y_train) 
y_train_pred, y_test_pred = model.predict(X_train), model.predict(X_test) 
y_train_prob, y_test_prob = model.predict_proba(X_train)[:, 1], model.predict_proba(X_test)[:, 1] 
# Evaluation 
print(f"Train Loss: {log_loss(y_train, y_train_prob):.4f}, Accuracy: {accuracy_score(y_train, y_train_pred)*100:.2f}%") 
print(f"Test  Loss: {log_loss(y_test, y_test_prob):.4f}, Accuracy: {accuracy_score(y_test, y_test_pred)*100:.2f}%") 
# Decision boundary plot 
xx, yy = np.meshgrid(np.linspace(X[:,0].min()-1, X[:,0].max()+1, 100), 
np.linspace(X[:,1].min()-1, X[:,1].max()+1, 100)) 
grid = scaler.transform(np.c_[xx.ravel(), yy.ravel()]) 
probs = model.predict_proba(grid)[:,1].reshape(xx.shape) 
plt.contourf(xx, yy, probs, levels=20, cmap="coolwarm", alpha=0.6) 
plt.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap="coolwarm", edgecolors='k') 
plt.title("Logistic Regression Decision Boundary") 
plt.xlabel("Feature 1") 
plt.ylabel("Feature 2") 
plt.colorbar(label="Probability") 
plt.show()
